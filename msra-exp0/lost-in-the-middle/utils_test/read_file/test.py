# from transformers import AutoModelForCausalLM
# from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
import sys
sys.path.append('/home/LeiFeng/yixing/conda/anaconda3/envs/lost-in-the-middle/lib/python3.9/site-packages/transformers/models/llama')
# from ...utils import  add_start_docstrings
import longchat
print(f'{longchat.__file__}')
